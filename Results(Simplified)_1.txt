Definition:
Evaluation: detailed account of class evaluation method, update method, and sample classification.
Config: config of trial(D, iter_per_update, iterations, drop rate)
	- Dataset is smart_home by default.
	- restart/adjusted/resume learning: upon update, "restart" reinitializes the whole model, "adjust" reinitializes the dropped indices, "resume" does nothing
pre test: basically baseline
test	: accuracies during training with update
post test: use the final basis and start afresh 
fin	: after training with update reaches max epoch limit, train until convergence (or max iteration limit, which is usually the case)

Eval: 	normalized variance: each class vector is normailzed by sqrt of update counts and then the variances of each HD feature is calculated
{
Config: D = 1000; iter_p_u = 5; iter# = 200;
drop_rate = 0.2; restart learning
Max pre test acc: 	 96.322210 	
Max test acc: 	 	 97.757445 	
Max post test acc: 	 98.223897 	
Max fin	 acc: 	 	 98.116254 

Config: D = 500; iter_p_u = 5; iter# = 200.
drop_rate = 0.2; restart learning
Max pre test acc: 	 93.774668 	
Max test acc: 	 	 97.201292 	
Max post test acc: 	 97.219232 	
Max fin	 acc: 	 	 97.255113 


						# Difference between restart, adjusted, resume learning, part 1.
Config: D = 500; iter_p_u = 5; iter# = 200.
drop_rate = 0.2; restart learning
Max pre test acc: 	 94.330822 	
Max test acc: 	 	 97.147470 	
Max post test acc: 	 97.649803 	
Max fin	 acc: 	 	 97.542160 

Config: D = 500; iter_p_u = 5; iter# = 200.
drop_rate = 0.2; adjusted learning
Max pre test acc: 	 94.815213 	
Max test acc: 	 	 96.358091 	
Max post test acc: 	 96.555436 	
Max fin	 acc: 	 	 96.340151

Config: D = 500; iter_p_u = 5; iter# = 200.
drop_rate = 0.2; resume learning
Max pre test acc: 	 94.922856 	
Max test acc: 	 	 96.609257 	
Max post test acc: 	 96.573376 	
Max fin	 acc: 	 	 96.519555 
}


						# Weighted eval
Eval: 	normalized variance: each class vector is normailzed by sqrt of update counts and then the variances of each HD feature is calculated
	weighted updates - naive: updating each HD feature with weights = (the last update it got updated + 1) (original = 1, first update = 2)
{
Config: D = 500; iter_p_u = 5; iter# = 200.
drop_rate = 0.2; adjusted learning
Max pre test acc: 	 94.743452 	
Max test acc: 	 	 97.021887 	
Max post test acc: 	 97.165411 	
Max fin	 acc: 	 	 96.914245 	
Another:
Max pre test acc: 	 94.259060 	
Max test acc: 	 	 97.308934 	
Max post test acc: 	 97.290994 	
Max fin	 acc: 	 	 97.326875
Another:
Max pre test acc: 	 94.994618 	
Max test acc: 	 	 97.255113 	
Max post test acc: 	 97.290994 	
Max fin	 acc: 	 	 97.290994 
(Special note: Appendix B contains more info on updating for this trial, including the last time an HD feature got updated and the total amount of time it got updated)	

						# D down to 200
Same but for D = 200:
Max pre test acc: 	 89.074273 	
Max test acc: 	 	 95.407248 	
Max post test acc: 	 95.514891 	
Max fin	 acc: 	 	 95.783997 	

Same but for D = 200 + MNIST:
Max pre test acc: 	 88.510000 	
Max test acc: 	 	 91.000000 	
Max post test acc: 	 91.020000 	
Max fin	 acc: 	 	 91.510000
}

Eval: 	normalized variance
	weighted updates - naive: updating each HD feature with weights = (the last update it got updated + 1) (original = 1, first update = 2)
	masked evaluation: during training, masked the best indices for sample classification. (And not update it)
{
Config: D = 200; iter_p_u = 5; iter# = 200.
drop_rate = 0.2; adjusted learning
Max pre test acc: 	 89.325440 	
Max test acc: 	 	 89.881593 	
Max post test acc: 	 94.851094 	
Max fin	 acc: 	 	 94.994618 
}

Eval: 	normalized variance
	weighted updates - naive: updating each HD feature with weights = (the last update it got updated + 1) (original = 1, first update = 2)
		- Alternative learning rate
	masked evaluation: during training, masked the best indices for sample classification. (But still update it)
2 3 3 2 1
w = 11111
12111	(5, 5, 5, 5, 5) --> (5, 0, 5, 5, 5)
12311	()
12411
15411
65411

cl = cl + sample * w 


{
Config: D = 200; iter_p_u = 5; iter# = 200.
drop_rate = 0.2; adjusted learning

Max pre test acc: 	 88.500179 	
Max test acc: 	 	 90.724794 	
Max post test acc: 	 95.209903 
Max fin	 acc: 	 	 95.335486 
Another:
Max pre test acc: 	 88.751346 	
Max test acc: 	 	 90.348044 	
Max post test acc: 	 95.084320 	
Max fin	 acc: 	 	 95.227844 
(Special note: test accuracy during training seems to fluctuate greatly between ~85 and ~60 while train increases consistently. See Appendix A.)

Config: D = 200; iter_p_u = 5; iter# = 10.
drop_rate = 0.2; adjusted learning
Max pre test acc: 	 90.276283 	
Max test acc: 	 	 85.970578 	
Max post test acc: 	 91.334769 	
Max fin	 acc: 	 	 91.711518 
					# Difference between restart, adjusted, resume learning, part 2.
Config: D = 200; iter_p_u = 5; iter# = 20.
drop_rate = 0.2; adjusted learning
Max pre test acc: 	 89.289559 	
Max test acc: 	 	 89.217797 	
Max post test acc: 	 92.985289 	
Max fin	 acc: 	 	 93.021170

Config: D = 200; iter_p_u = 5; iter# = 200.
drop_rate = 0.2; resume learning
Max pre test acc: 	 87.656979 	
Max test acc: 	 	 91.173305 	
Max post test acc: 	 94.976677 	
Max fin	 acc: 	 	 95.353427 

Config: D = 200; iter_p_u = 5; iter# = 200.
drop_rate = 0.2; restart learning
Max pre test acc: 	 88.805167 	
Max test acc: 	 	 84.212415 	
Max post test acc: 	 84.804449 	
Max fin	 acc: 	 	 84.266236
}
	
NEED: 
	early stopping: accuracy may degrade after a while. Add validation set to track "test accuracy" for early stopping
		- It is shown that though early trials display relatively high test accuracy, the base is bad 
	Concerns: well it's actually not always degrading but fluctuating. So early stopping can be limited and more interesting method is needed.

Appendix A: Train accuracy ######################################################################################################################
78.27 	 86.89 	
75.70 	 83.14 	
78.20 	 83.84 	
78.25 	 86.53 	
78.70 	 84.55 	
78.81 	 83.26 	
78.83 	 85.97 	
80.78 	 84.97 	
81.52 	 87.51 	
82.09 	 87.82 	
82.32 	 84.86 	
80.71 	 85.09 	
82.48 	 88.86 	
83.17 	 83.55 	
83.62 	 86.94 	
83.52 	 84.91 	
81.33 	 80.53 	
83.20 	 75.40 	
83.67 	 80.53 	
83.54 	 72.59 	
83.98 	 81.45 	
82.19 	 77.63 	
83.91 	 84.30 	
84.19 	 76.26 	
84.36 	 85.81 	
84.92 	 79.98 	
83.74 	 83.33 	
84.81 	 75.37 	
85.63 	 81.20 	
85.45 	 86.60 	
85.84 	 81.36 	
84.62 	 86.69 	
85.56 	 84.73 	
86.00 	 86.92 	
86.17 	 86.98 	
86.27 	 87.01 	
83.54 	 82.22 	
84.61 	 83.01 	
85.01 	 85.29 	
85.28 	 80.50 	
84.91 	 86.01 	
85.29 	 71.35 	
85.83 	 74.92 	
86.47 	 73.95 	
86.88 	 75.67 	
86.95 	 72.44 	
86.04 	 79.83 	
86.55 	 75.62 	
87.09 	 75.92 	
86.99 	 76.77 	
87.35 	 80.89 	
85.60 	 75.26 	
86.55 	 79.83 	
86.94 	 82.26 	
87.11 	 82.47 	
87.51 	 85.79 	
85.91 	 76.84 	
86.94 	 77.99 	
86.78 	 78.27 	
87.10 	 78.81 	
87.30 	 74.02 	
85.97 	 68.19 	
86.73 	 66.16 	
87.51 	 71.19 	
87.36 	 71.37 	
87.50 	 70.15 	
84.97 	 66.52 	
86.30 	 72.03 	
86.98 	 67.08 	
86.59 	 70.00 	
86.81 	 65.90 	
85.78 	 77.02 	
86.92 	 79.55 	
86.94 	 79.78 	
87.19 	 79.28 	
87.48 	 70.13 	
86.88 	 79.67 	
87.55 	 85.81 	
87.74 	 83.33 	
87.65 	 83.05 	
88.05 	 82.94 	
87.41 	 73.04 	
88.03 	 73.99 	
88.22 	 74.83 	
88.31 	 71.22 	
88.55 	 74.31 	
87.02 	 80.12 	
88.22 	 80.68 	
88.28 	 82.29 	
88.63 	 79.83 	
88.44 	 86.35 	
86.46 	 81.77 	
87.22 	 83.10 	
87.59 	 75.96 	
87.76 	 75.73 	
88.09 	 79.98 	
87.78 	 71.22 	
88.30 	 71.28 	
89.11 	 69.77 	
88.64 	 65.25 	
88.83 	 71.42 	
86.19 	 77.68 	
87.42 	 78.24 	
87.49 	 74.99 	
87.44 	 75.22 	
87.79 	 76.25 	
86.89 	 81.32 	
87.81 	 78.35 	
88.31 	 83.91 	
88.31 	 84.88 	
88.51 	 81.84 	
87.50 	 76.53 	
88.88 	 72.17 	
88.91 	 80.57 	
89.05 	 85.47 	
89.15 	 81.02 	
87.94 	 73.48 	
88.87 	 75.94 	
89.26 	 76.46 	
89.07 	 73.77 	
89.83 	 76.23 	
87.41 	 72.62 	
88.44 	 73.50 	
88.70 	 74.17 	
88.81 	 72.14 	
88.74 	 73.63 	
87.30 	 66.63 	
88.28 	 69.12 	
88.11 	 69.02 	
88.28 	 69.86 	
88.40 	 70.63 	
87.63 	 78.45 	
88.20 	 84.23 	
88.62 	 79.83 	
88.58 	 79.37 	
88.50 	 81.04 	
87.78 	 85.49 	
88.61 	 82.99 	
88.76 	 84.52 	
88.88 	 82.44 	
88.80 	 84.91 	
87.72 	 82.92 	
88.31 	 83.06 	
88.67 	 82.08 	
88.56 	 82.85 	
88.92 	 81.74 	
88.17 	 70.88 	
88.75 	 71.62 	
89.06 	 71.82 	
89.12 	 72.17 	
89.28 	 73.47 	
88.39 	 70.13 	
89.11 	 68.19 	
89.21 	 64.76 	
89.31 	 70.25 	
89.56 	 69.54 	
89.17 	 65.50 	
89.59 	 67.49 	
90.09 	 64.24 	
89.72 	 62.81 	
89.92 	 61.89 	
88.96 	 62.15 	
89.58 	 62.94 	
89.63 	 60.69 	
89.72 	 61.41 	
89.98 	 64.42 	
88.02 	 69.54 	
89.02 	 70.97 	
89.29 	 71.85 	
89.53 	 70.15 	
89.41 	 64.50 	
88.36 	 73.48 	
88.73 	 72.91 	
89.40 	 74.09 	
89.22 	 70.90 	
89.33 	 72.10 	
88.61 	 71.13 	
89.26 	 71.73 	
89.48 	 73.09 	
89.80 	 72.01 	
89.52 	 70.86 	
89.18 	 64.76 	
89.91 	 71.10 	
90.37 	 67.13 	
90.11 	 70.11 	
90.21 	 72.55 	
89.15 	 79.67 	
89.80 	 79.64 	
89.80 	 75.48 	
90.12 	 73.07 	
89.98 	 75.64 	
90.01 	 88.70 	
90.30 	 85.92 	
90.70 	 90.35 	
90.73 	 84.89 	
91.00 	 88.48 	
89.46 	 80.27 	
90.26 	 87.10 	
90.14 	 82.42 	
90.38 	 84.86 	
90.29 	 82.24


Appendix B: Update detail ######################################################################################################################
Last update: [ 8. 41. 40.  1.  7.  5.  4.  1. 16.  4. 12.  8. 17. 15.  2. 39.  2.  8.
 17. 16. 39. 41. 41.  5. 30.  7. 31. 12.  9.  1. 41.  4.  9.  7.  6. 21.
 32. 24. 41. 10. 11. 41. 22. 41. 19.  1.  4. 41.  5. 36.  1.  4.  3. 33.
 41. 41. 41.  1.  6.  4. 41. 21. 26. 41.  4.  7.  1. 10. 17. 10.  9. 16.
 41.  1. 27. 41.  5. 23. 41.  1.  1. 41. 41. 41. 15.  2.  1.  5.  1.  4.
 41.  5. 34.  1. 29. 36.  1. 34. 14.  3. 41.  2.  1. 20. 41. 10.  2. 41.
  1. 11.  3.  7. 14. 23.  7. 41. 20. 20. 16. 18. 41. 16.  5. 28. 40. 16.
 21.  1.  4. 13. 41.  1. 25. 17.  8. 41.  4. 27.  6.  1. 41. 41.  8. 14.
 41. 25. 34.  8.  1.  5. 17. 24. 33.  3. 21. 12. 33.  1. 16.  2. 14.  4.
 28.  6.  8.  6.  9.  1. 10. 36. 29.  2. 41.  4. 41.  1. 41. 36.  1. 11.
  9.  8. 41. 27. 24.  4. 22.  3. 32.  4. 34.  8.  1. 20. 41.  4.  6. 39.
 41.  8. 41. 41. 36.  5. 13. 29.  9. 35. 15. 41.  1.  2.  8. 41.  5. 41.
 41. 41. 41. 15.  4. 12. 41. 11. 41.  5.  4.  8.  3.  1.  3. 35. 41. 41.
 41. 10. 18. 40. 13. 15.  8.  5.  6. 41. 19. 32. 11.  1. 19. 31.  6.  1.
  1.  4. 40. 19. 23. 14. 35.  2.  4. 41. 41. 41. 41. 41. 11. 19. 30. 41.
 22. 28.  5. 41. 41.  5. 41.  1.  7.  7. 17. 15. 41. 14.  3. 10. 41.  5.
  2. 11. 28.  3.  8.  7.  1. 41. 14.  1. 15.  4. 13. 12. 14. 14. 41. 17.
  2.  4. 41. 19. 16. 41. 24. 12. 41. 26. 41. 40. 10. 41. 41.  7. 11. 26.
 20. 41.  8. 41.  1.  6. 41.  1. 37.  1. 15. 38.  3.  8.  2.  2. 30.  5.
 22.  9.  7. 24. 16. 14. 41. 28.  8.  4. 11.  6.  4. 23. 15. 25. 21.  8.
 18.  4. 27.  7.  1. 22.  2. 26. 12. 12. 18.  5.  3.  3.  8. 41. 41. 34.
  1.  4. 41. 14.  1.  1. 39. 41. 32. 41. 41.  6.  3.  3. 24. 36. 31. 41.
 27.  2. 40.  3.  1. 41. 41. 10. 11. 11. 41.  1.  7. 19. 41. 15.  7.  6.
 26. 39. 12.  6.  1. 21.  5. 20. 41.  1. 41. 14.  5.  3. 41. 20. 25. 14.
  7. 41. 10.  8.  9.  1. 19. 19. 18. 19. 30. 15. 23.  9.  6.  4. 38. 29.
  9. 41. 28.  2.  8.  4. 32.  9. 13. 41.  1. 41.  8.  5. 41. 41.  1. 10.
  3. 26.  3.  5.  5. 34. 41. 11. 12. 41. 41. 30. 37. 28.  1.  3. 41.  1.
 41. 21.  8.  2.  9. 27.  8. 22.  1. 41.  7. 33.  3. 41.]
Update counts:
[ 1. 14. 23.  0.  1.  2.  1.  0.  6.  1.  7.  2.  7.  1.  1. 10.  1.  1.
 12.  8. 12. 21.  2.  2.  8.  2. 16.  4.  1.  0. 22.  1.  1.  4.  2. 11.
 19.  8. 10.  7.  3. 24. 13. 22.  2.  0.  1.  8.  1. 13.  0.  2.  2. 19.
 29. 16. 20.  0.  2.  1. 26.  7.  8.  6.  2.  3.  0.  4. 10.  2.  7.  2.
 14.  0. 14.  6.  2. 13.  8.  0.  0.  5.  8.  6.  9.  1.  0.  3.  0.  1.
 15.  3. 24.  0.  7. 18.  0. 24.  8.  1. 12.  1.  0.  5. 20.  5.  1. 32.
  0.  3.  1.  3. 12.  2.  4.  9.  8.  5. 13.  4. 23. 12.  2.  7. 20.  8.
  3.  0.  2.  4. 18.  0. 16. 10.  1. 37.  2. 22.  2.  0. 13. 27.  2.  1.
 24. 15. 14.  6.  0.  2.  9. 12.  2.  1.  7.  6. 13.  0.  3.  1.  4.  1.
 16.  3.  5.  3.  4.  0.  3. 28. 19.  1. 23.  3. 36.  0. 13.  2.  0.  1.
  4.  4. 28. 17.  9.  2. 17.  1.  9.  2. 12.  4.  0.  5. 17.  2.  3. 21.
 24.  5. 27.  1. 24.  1.  8. 12.  5. 14.  2. 15.  0.  1.  2. 31.  2. 20.
 33. 32. 30.  2.  2. 10. 29.  7. 16.  2.  2.  2.  1.  0.  1.  8.  1. 27.
 16.  3.  8. 19.  5.  4.  4.  1.  1. 35. 10.  4.  4.  0. 12. 22.  4.  0.
  0.  2.  6.  2.  7.  7. 28.  1.  2.  8. 37. 32.  2. 27.  7.  3.  5.  3.
 13. 18.  2. 11. 21.  3. 10.  0.  6.  2. 11.  9. 10.  2.  1.  3. 33.  2.
  1.  1. 13.  2.  6.  3.  0. 23.  5.  0.  5.  1.  5.  1.  7.  5. 28.  1.
  1.  1. 20.  8.  5. 27.  4.  3. 30.  7. 35. 24.  3. 14. 32.  1.  6. 20.
  1. 19.  2. 31.  0.  2. 32.  0. 24.  0.  1. 11.  2.  1.  1.  1.  3.  1.
 15.  2.  1. 12.  1.  9. 10.  4.  5.  1.  5.  4.  1. 13.  8. 15.  1.  2.
  5.  2.  3.  3.  0.  7.  1.  3.  4.  4.  9.  1.  2.  1.  1. 34.  1. 26.
  0.  2. 16.  6.  0.  0. 10.  5. 12. 30. 28.  1.  1.  1. 13. 20. 21. 12.
  7.  1. 12.  1.  0. 15. 11.  4.  2.  4. 16.  0.  3.  5.  4. 10.  4.  3.
 18.  4.  2.  2.  0.  1.  1.  8. 13.  0. 18.  6.  2.  1. 34.  9. 21.  2.
  1.  3.  2.  3.  4.  0. 11. 13.  3.  3. 25.  6.  9.  1.  2.  2. 19. 13.
  7. 12.  4.  1.  1.  2.  7.  3.  4. 21.  0. 24.  2.  2. 11. 13.  0.  5.
  1.  2.  1.  2.  2. 10. 22.  3.  4. 21. 10. 13. 22. 15.  0.  1.  9.  0.
 29.  3.  3.  1.  1. 16.  3.  4.  0.  9.  2. 25.  1. 15.]
